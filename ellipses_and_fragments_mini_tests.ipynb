{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import itertools\n",
    "import argparse\n",
    "from klm.query import slor, LM, get_unigram_probs\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "with open(\"gists/minidogs.jsonl\", \"r\") as inf:\n",
    "    lns = [json.loads(i) for i in inf]\n",
    "    \n",
    "from environments.envs import ENVIRONMENTS\n",
    "\n",
    "env = ENVIRONMENTS[\"DEV\"]\n",
    "LOC = env['klm_model']\n",
    "UG_MODEL = env[\"ug_model\"]\n",
    "\n",
    "lm = LM(loc=LOC)\n",
    "up = get_unigram_probs(UG_MODEL)\n",
    "\n",
    "ROOT = 'ROOT'\n",
    "\n",
    "\n",
    "def get_all_combos_of_constituents(jdoc, max_c=100, p=.75):\n",
    "    '''\n",
    "    If you assume that the linguistic units are constituents and assume that you want to fill at least \n",
    "    p percent of your budget and that you want no more than K elllipses... this is actually combinatorily\n",
    "    tractable FOR A SINGLE SENTENCE\n",
    "    \n",
    "    You just want all nCk combos of constituents, for 1 .... K + 1.\n",
    "    \n",
    "    - Then you exclude all of the combos w len more than b and less than p * b\n",
    "    - You also exclude cases where one unit is nested in the other. You can exclued these safely I think b/c\n",
    "      if there is a set (x1, x2... xn) s.t xi \\in xj then there will be \n",
    "      another set (x1, x2... xn) that includes all but xi that makes better use of the xi position\n",
    "    '''\n",
    " \n",
    "    out = list()\n",
    "    tree2 = Tree.fromstring(jdoc[\"parse\"])\n",
    "    getNodes(tree2, out)\n",
    "\n",
    "    tm = [p[1] for p in out]\n",
    "\n",
    "    co = 0\n",
    "    opts = list()\n",
    "    def has_overlap(tm):\n",
    "        for a,b in itertools.combinations(tm, r=2):\n",
    "            if a in b or b in a:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    for i in range(1,4):\n",
    "        for v in itertools.combinations(tm, r=i):\n",
    "            co += 1\n",
    "            if sum([len(i) for i in v]) < max_c and sum([len(i) for i in v]) > (p * max_c):\n",
    "                if not has_overlap(v):\n",
    "                    opts.append(list(v))\n",
    "    return opts\n",
    "\n",
    "\n",
    "def combo2snippet(jdoc,combos):\n",
    "    sent = \" \".join([o[\"word\"] for o in jdoc[\"tokens\"]])\n",
    "    combos.sort(key=lambda x:sent.index(x))\n",
    "    return(\"...\".join([c for c in combos]))\n",
    "\n",
    "\n",
    "def getNodes(parent, out):\n",
    "    for node in parent:\n",
    "        if type(node) is nltk.Tree:\n",
    "            if node.label() == ROOT:\n",
    "                print(\"======== Sentence =========\")\n",
    "                print(\"Sentence:\", \" \".join(node.leaves()))\n",
    "            else:\n",
    "                if len(node.leaves()) > 1:\n",
    "                    out.append((node.label(), \" \".join(node.leaves())))\n",
    "\n",
    "            getNodes(node, out)\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "\n",
    "def find_ngrams(input_list, n):\n",
    "    return zip(*[input_list[i:] for i in range(n)])\n",
    "\n",
    "\n",
    "def insert_all_ellipses(toks, start_index, end_index):\n",
    "    '''\n",
    "    \n",
    "    here is another thing that does not really work. \n",
    "    \n",
    "    assume you have  sequence of tokens. Now you want to insert an elipse somewhere. This returns all of the \n",
    "    places where you can get an ellipse. \n",
    "    \n",
    "    This is quadratic b/c you loop over start and ends of elipses. This cuts\n",
    "    down on the search complexity of compressive summarization problem\n",
    "    \n",
    "    If you made K recursive calls to add more ellipses, I bet you'd find that high ranking cases mapped to constituents, kind of.\n",
    "    \n",
    "    This is something we could use to justify the constituent assumption tho\n",
    "    \n",
    "    More details: \n",
    "\n",
    "    start_index(int): where to start looping, to insert ellipse\n",
    "    end_index(int): where to end looping, to insert ellipse\n",
    "         - If you are looping over the whole sentences then \n",
    "             - start_index = 0 and end_index = len(toks)\n",
    "         - But also you might loop over the last 5 tokens in the sentence\n",
    "    \n",
    "    Returns a list of tuples. \n",
    "        - The first item in the tuple is the tokens included \n",
    "        - The second item in the tuple is the start of ellipse\n",
    "        - The third item in the tuple is the end of ellipse\n",
    "    '''\n",
    "    for start_ellipse in range(start_index, end_index):\n",
    "        for end_ellipse in range(start_ellipse + 1, end_index):\n",
    "            yield(start_ellipse, end_ellipse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does SLOR do a good job finding well-formed ngrams? \n",
    "No. Sort of disappointed. I guess the coarse ranking is good but fine-grained is not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('thirty min walks a day .', 0.9980161390218095)\n",
      "('My little couch potato dog needs', 0.6113157377929687)\n",
      "('3 thirty min walks a day', 0.4164939557576499)\n",
      "('little couch potato dog needs 3', 0.3197258045450842)\n",
      "('couch potato dog needs 3 thirty', 0.031799057637532734)\n",
      "('potato dog needs 3 thirty min', 0.03132666356608086)\n",
      "('dog needs 3 thirty min walks', 0)\n",
      "('needs 3 thirty min walks a', 0)\n"
     ]
    }
   ],
   "source": [
    "index = 26\n",
    "all_ = []\n",
    "for o in find_ngrams([o[\"word\"] for o in lns[index]['tokens']], n=6):\n",
    "    str_ = \" \".join(o)\n",
    "    all_.append((str_, max(slor(str_,lm, up), 0)))\n",
    "    \n",
    "all_.sort(key=lambda x:x[1], reverse=True)\n",
    "\n",
    "for o in all_:\n",
    "    print(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does SLOR do a good job finding well-formed constituents? \n",
    "- No. Sort of disappointed. The coarse ranking is good but not the fine-grained ranking, basically. This fits w/ Lau Clarke Lapin who found an approximate correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Tree' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-749fdd662ea6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mK\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msno\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m26\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtree2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromstring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msno\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"parse\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mgetNodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Tree' is not defined"
     ]
    }
   ],
   "source": [
    "out = []\n",
    "opts = []\n",
    "K = 20\n",
    "sno = 26\n",
    "tree2 = Tree.fromstring(lns[sno][\"parse\"])\n",
    "getNodes(tree2, out)\n",
    "for o in out:\n",
    "    label, frag = o\n",
    "    if len(frag.split()) < K:\n",
    "        opts.append((frag, slor(frag,lm, up)))\n",
    "\n",
    "opts = list(set(opts))\n",
    "opts.sort(key=lambda x:x[1],reverse=True)\n",
    "\n",
    "print(\" \".join([o[\"word\"] for o in lns[sno][\"tokens\"]]))\n",
    "for o in opts:\n",
    "    print(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A method that works\n",
    "\n",
    "Assume you want only a few ellipses (2/3) per sentence. Assume you want to fill your summary budget up to 75%. You can really just enumerate all combos of constituents of size 1,2,3,4, and filter out those that are too long and too short. This is because the number of ellipses will in general be size of combo + 1. There are a manageable number of subsets to just check. If we rank the resullting snippets by SLOR, imagining that the ... are not present, well everything seems to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "sno = 35\n",
    "combos = get_all_combos_of_constituents(lns[sno])\n",
    "\n",
    "ranks = []\n",
    "\n",
    "for c in combos:\n",
    "    snip = combo2snippet(lns[sno], c)\n",
    "    \n",
    "    ranks.append((snip, slor(snip.replace(\"...\", \" \").replace(\"  \", \" \"),lm, up)))\n",
    "    \n",
    "ranks.sort(key=lambda x:x[1], reverse=True)\n",
    "\n",
    "print(\"top 10 snippets, by SLOR\")\n",
    "for r in ranks[0:10]:\n",
    "    print(r)\n",
    "print(\"\\n\")\n",
    "print(\"bottom 10 snippets, by SLOR\")\n",
    "for r in ranks[-10:]:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One note\n",
    "\n",
    "if you insert ellipses w/o respecting constituents. The SLOR ranking seems ok-ish to evaluate different summaries of\n",
    "the same size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from environments.envs import ENVIRONMENTS\n",
    "from collections import defaultdict\n",
    "import argparse\n",
    "import kenlm\n",
    "import json\n",
    "\n",
    "from klm.query import LM\n",
    "from klm.query import get_unigram_probs\n",
    "\n",
    "env = ENVIRONMENTS[\"DEV\"]\n",
    "LOC = env['klm_model']\n",
    "UG_MODEL = env[\"ug_model\"]\n",
    "\n",
    "lm = LM(loc=LOC)\n",
    "up = get_unigram_probs(UG_MODEL)\n",
    "from klm.query import slor\n",
    "\n",
    "\n",
    "toks = \"my little couch potato needs 3 thirty minute walks\".split()\n",
    "\n",
    "opts = []\n",
    "\n",
    "classes = defaultdict(list)\n",
    "\n",
    "for i in insert_all_ellipses(toks, start_index=0, end_index=len(toks) + 1):\n",
    "    ellipse_min, ellipse_max = i\n",
    "    printthis = \"\"\n",
    "    for tno, t in enumerate(toks):\n",
    "        if tno == ellipse_min:\n",
    "            printthis = printthis + \" ... \"\n",
    "        elif tno > ellipse_min and tno < ellipse_max:\n",
    "            pass\n",
    "        else:\n",
    "            printthis = printthis + \" \" + t\n",
    "    str_ = printthis.replace(\" ... \", \" \").strip().replace(\"  \", \" \")\n",
    "    classes[len(str_.split())].append((printthis, slor(str_,lm, up)))\n",
    "    \n",
    "for c in classes:\n",
    "    print(c)\n",
    "    opts = classes[c]\n",
    "    opts.sort(key=lambda x:x[1], reverse=True)\n",
    "    opts[0:20]\n",
    "    for o in opts:\n",
    "        print(o)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
